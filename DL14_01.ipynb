{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "91b14c87",
   "metadata": {},
   "source": [
    "# 数值稳定性\n",
    "### 神经网络的梯度\n",
    "* 考虑如下有d层的神经网络\n",
    "![d层网络](pic/1401.png)\n",
    "* 计算损失$l$关于参数$W_t$的梯度(d-t次矩阵乘法)\n",
    "![d层网络梯度](pic/1402.png)\n",
    "### 数值稳定性的常见问题\n",
    "* 梯度爆炸\n",
    "* 梯度消失\n",
    "### 例子:MLP\n",
    "* 加入如下MLP(为了简单省略了偏移b)\n",
    "![1403](pic/1403.png)\n",
    "![1403](pic/1404.png)\n",
    "![1403](pic/1405.png)\n",
    "#### 梯度爆炸\n",
    "* 使用ReLU作为激活函数\n",
    "![1403](pic/1406.png)\n",
    "如果d-t很大,值会非常大\n",
    "#### 梯度爆炸的问题\n",
    "* 值超出值域(infinity)\n",
    "    * 对于16位浮点数尤为严重(数值区间6e-5 - 6e4)\n",
    "* 对学习率($\\eta$)非常敏感\n",
    "    * 如果学习率太大->大参数值->更大的梯度\n",
    "    * 如果学习率太小->训练无进展\n",
    "    * 我们可能需要在训练过程中不断调整学习率\n",
    "#### 梯度消失\n",
    "* 使用sigmoid作为激活函数\n",
    "![1403](pic/1407.png\n",
    "如果输入很大,梯度会非常小\n",
    "#### 梯度消失的问题\n",
    "* 梯度值变成0\n",
    "    * 对16位浮点数尤为严重\n",
    "* 训练没有进展\n",
    "    * 不管如何选择学习率\n",
    "* 对于底层尤为严重\n",
    "    * 仅仅顶部层训练的较好\n",
    "    * 无法让神经网络更深"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44978035",
   "metadata": {},
   "source": [
    "## 总结\n",
    "* 当数值过大或者过小的时候会导致数值\n",
    "* 常发生在深度模型在,因为会对n个数累乘"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
