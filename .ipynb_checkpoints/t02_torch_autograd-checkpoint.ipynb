{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Introduction of torch.autograd\n",
    "torch.autograd is PyTorch’s automatic differentiation\n",
    "engine 自动差分引擎 that powers neural network training.\n",
    "In this section, you will get a conceptual\n",
    "understanding of how autograd helps a neural network\n",
    "train."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Background\n",
    "Neural networks (NNs) are a collection of nested\n",
    "functions that are executed on some input data.\n",
    "These functions are defined by *parameters* (consisting\n",
    "of weights and biases 由权重和偏差组成), which is in\n",
    "PyTorch are stored in tensors.\n",
    "\n",
    "Training a NN happens in two steps:<br>\n",
    "**Forward Propagation 正向传播**: In forward prop, the\n",
    "NN makes its best guess about the correct output. It\n",
    "runs the input data through each of its functions to\n",
    "make this guess<br>\n",
    "**Backward Propagation**: In back prop, the NN adjusts\n",
    "its parameters proportionate 误差 to the error in its\n",
    "guess. It does this by traversing backwards from the\n",
    "output, collecting the derivatives of the error 误差导数\n",
    "with respect to the parameters of the functions (gradient),\n",
    "and optimizing the parameters using gradient descent\n",
    "梯度下降. For a more detailed walk-through of backprop,\n",
    "check out this [video](https://www.youtube.com/watch?v=tIeHLnjs5U8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Usage in PyTorch\n",
    "Let's take a look at a single training step. For this example,\n",
    "we load a pretrained resnet18 model from *torchvision*. We\n",
    "create a random data tensor to represent a single image\n",
    "with 3 channels, and height & width of 64, and its correspond\n",
    "*label* initialized to some random values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading: \"https://download.pytorch.org/models/resnet18-f37072fd.pth\" to C:\\Users\\Yueqiao/.cache\\torch\\hub\\checkpoints\\resnet18-f37072fd.pth\n",
      "100.0%\n"
     ]
    }
   ],
   "source": [
    "import torch, torchvision\n",
    "\n",
    "model = torchvision.models.resnet18(pretrained=True)\n",
    "data = torch.rand(1, 3, 64, 64)\n",
    "labels = torch.rand(1, 1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Next, we run the input data through the model through each\n",
    "of its layers to make a prediction. This is the **forward\n",
    "pass**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Yueqiao\\anaconda3\\envs\\PyTorchEnv\\lib\\site-packages\\torch\\nn\\functional.py:718: UserWarning: Named tensors and all their associated APIs are an experimental feature and subject to change. Please do not use them for anything important until they are released as stable. (Triggered internally at  ..\\c10/core/TensorImpl.h:1156.)\n",
      "  return torch.max_pool2d(input, kernel_size, stride, padding, dilation, ceil_mode)\n"
     ]
    }
   ],
   "source": [
    "prediction = model(data)  # forward pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "We use the model's prediction, and the corresponding label\n",
    "to calculate the error 误差 (loss). The next step is to\n",
    "back propagate this error through the network. Backward\n",
    "propagation is kicked off when we call *.backward()* on the\n",
    "error tensor. Autograd then calculates and stores the\n",
    "gradients for each model parameter in the parameter's\n",
    "*.grad* attribute. 然后，Autograd 会为每个模型参数计算梯度\n",
    "并将其存储在参数的.grad属性中"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "loss = (prediction - labels).sum()\n",
    "loss.backward()  # backward pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Next, we load an optimizer, in this case SGD with a learning\n",
    "rate of 0.01 and momentum 动量 of 0.9. We register all the\n",
    "parameters of the model in the optimizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "optim = torch.optim.SGD(model.parameters(), lr=1e-2,\n",
    "                        momentum=0.9)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Finally, we call *.step()* to initiate gradient descent.\n",
    "The optimizer adjusts each parameter by its gradient\n",
    "stored in *.grad*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "optim.step()  # gradient descent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "At this point, you have everything you need to train your\n",
    "neural network. The below sections detail the workings of\n",
    "autograd - feel free to skip them.\n",
    "****"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Differentiation in Autograd\n",
    "Let's take a look at how *autograd* collects gradients. We\n",
    "create two tensors *a* and *b* with *requires_grad=True*.\n",
    "This signals to *autograd* that every operation on them\n",
    "should be tracked."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "a = torch.tensor([2., 3.], requires_grad=True)\n",
    "b = torch.tensor([6., 4.], requires_grad=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "We create another tensor Q from a and b\n",
    "$Q = 3a^3-b^2$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "Q = 3*a**3 - b**2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Let's assume a and b to be parameters of an NN, and Q to\n",
    "be the error. In NN training, we want gradients of the\n",
    "error w.r.t. parameters, i.e.\n",
    "\n",
    "$$\\frac{\\partial Q}{\\partial a} = 9a^2$$ <br>\n",
    "$$\\frac{\\partial Q}{\\partial b} = -2b^2$$\n",
    "\n",
    "When we call *.backward()* on Q, autograd calculates these\n",
    "gradients and stores them in the respective tensor's\n",
    "*.grad* attribute\n",
    "\n",
    "We need to explicitly 明确的 pass a *gradient* argument in\n",
    "*Q.backward()* because it is a vector *.gradient* is a\n",
    "tensor of same shape as Q, and it represents the gradient\n",
    "of Q w.r.t. itself, i.e.<br>\n",
    "我们需要在Q.backward()中显式传递gradient参数，因为它是向量。\n",
    "gradient是与Q形状相同的张量，它表示Q相对于本身的梯度，即\n",
    "\n",
    "$$\\frac{dQ}{dQ} = 1$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Equivalently 相当于, we can also aggregate 合计 Q into a\n",
    "scalar and call backward implicitly 暗中的, like\n",
    "*Q.sum().backward()*.<br>\n",
    "同样，我们也可以将Q聚合为一个标量，然后隐式地向后调用，\n",
    "例如Q.sum().backward()。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "external_grad = torch.tensor([1.,1.])\n",
    "Q.backward(gradient=external_grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Gradients are now deposited in *a.grad* and *b.grad* <br>\n",
    "梯度现在沉积在a.grad和b.grad中"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([True, True])\n",
      "tensor([True, True])\n"
     ]
    }
   ],
   "source": [
    "# check if collected gradients are correct\n",
    "print(9*a**2 == a.grad)\n",
    "print(-2*b == b.grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Optional Reading - Vector Calculus using autograd\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
