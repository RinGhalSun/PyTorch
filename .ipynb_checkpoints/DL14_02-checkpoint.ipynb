{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "09d87e2f",
   "metadata": {},
   "source": [
    "## 让训练更加稳定\n",
    "* 目标: 让梯度值在喝的范围内\n",
    "    * 例如[1e-6, 1e3]\n",
    "* 将乘法变加法\n",
    "    * ResNet, LSTM\n",
    "* 归一化\n",
    "    * 梯度归一化,梯度裁剪\n",
    "合理的权重初始和激活函数\n",
    "### 让每层方差是一个常数\n",
    "* 将每层的输出和梯度都看做随机变量\n",
    "* 让他们的均值和方差都保持一致\n",
    "### 权重初始化\n",
    "* 在合理值区间内随机初始参数\n",
    "* 训练开始时更容易有数值不稳定\n",
    "    * 远离最优解的地方损失函数可能很复杂\n",
    "    * 最优解附近表面会比较平\n",
    "* 使用N(0,0.01)来初始可能对小网络没问题,但不能保证深度网络"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "766bcbeb",
   "metadata": {},
   "source": [
    "### Xavier初始化\n",
    ":label:`subsec_xavier`\n",
    "\n",
    "让我们看看某些*没有非线性*的全连接层输出(例如，隐藏变量)$o_{i}$的尺度分布。\n",
    "对于该层$n_\\mathrm{in}$输入$x_j$及其相关权重$w_{ij}$，输出由下式给出\n",
    "\n",
    "$$o_{i} = \\sum_{j=1}^{n_\\mathrm{in}} w_{ij} x_j.$$\n",
    "\n",
    "权重$w_{ij}$都是从同一分布中独立抽取的。此外，让我们假设该分布具有零均值和方差$\\sigma^2$。请注意，这并不意味着分布必须是高斯的，只是均值和方差需要存在。现在，让我们假设层$x_j$的输入也具有零均值和方差$\\gamma^2$，并且它们独立于$w_{ij}$并且彼此独立。在这种情况下，我们可以按如下方式计算$o_i$的平均值和方差：\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "    E[o_i] & = \\sum_{j=1}^{n_\\mathrm{in}} E[w_{ij} x_j] \\\\&= \\sum_{j=1}^{n_\\mathrm{in}} E[w_{ij}] E[x_j] \\\\&= 0, \\\\\n",
    "    \\mathrm{Var}[o_i] & = E[o_i^2] - (E[o_i])^2 \\\\\n",
    "        & = \\sum_{j=1}^{n_\\mathrm{in}} E[w^2_{ij} x^2_j] - 0 \\\\\n",
    "        & = \\sum_{j=1}^{n_\\mathrm{in}} E[w^2_{ij}] E[x^2_j] \\\\\n",
    "        & = n_\\mathrm{in} \\sigma^2 \\gamma^2.\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "保持方差不变的一种方法是设置$n_\\mathrm{in} \\sigma^2 = 1$。现在考虑反向传播过程，我们面临着类似的问题，尽管梯度是从更靠近输出的层传播的。使用与正向传播相同的推理，我们可以看到，除非$n_\\mathrm{out} \\sigma^2 = 1$，否则梯度的方差可能会增大，其中$n_\\mathrm{out}$是该层的输出的数量。这使我们进退两难：我们不可能同时满足这两个条件。相反，我们只需满足：\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "\\frac{1}{2} (n_\\mathrm{in} + n_\\mathrm{out}) \\sigma^2 = 1 \\text{ 或等价于 }\n",
    "\\sigma = \\sqrt{\\frac{2}{n_\\mathrm{in} + n_\\mathrm{out}}}.\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "这就是现在标准且实用的*Xavier初始化*的基础，它以其提出者 :cite:`Glorot.Bengio.2010` 第一作者的名字命名。通常，Xavier初始化从均值为零，方差$\\sigma^2 = \\frac{2}{n_\\mathrm{in} + n_\\mathrm{out}}$的高斯分布中采样权重。我们也可以利用Xavier的直觉来选择从均匀分布中抽取权重时的方差。注意均匀分布$U(-a, a)$的方差为$\\frac{a^2}{3}$。将$\\frac{a^2}{3}$代入到$\\sigma^2$的条件中，将得到初始化的建议：\n",
    "\n",
    "$$U\\left(-\\sqrt{\\frac{6}{n_\\mathrm{in} + n_\\mathrm{out}}}, \\sqrt{\\frac{6}{n_\\mathrm{in} + n_\\mathrm{out}}}\\right).$$\n",
    "\n",
    "尽管上述数学推理中，不存在非线性的假设在神经网络中很容易被违反，但Xavier初始化方法在实践中被证明是有效的。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89e38233",
   "metadata": {},
   "source": [
    "## 总结\n",
    "* 合理的权重初始值和激活函数的选取可提升数值稳定性"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
